"use strict";(globalThis.webpackChunkgsf_docusaurus_template=globalThis.webpackChunkgsf_docusaurus_template||[]).push([[1445],{9157(e){e.exports=JSON.parse('{"tag":{"label":"role:data-scientist","permalink":"/gsf-patterns/tags/role-data-scientist","allTagsPath":"/gsf-patterns/tags","count":8,"items":[{"id":"catalog/development/pre-trained-transfer-learning","title":"Leverage pre-trained models and transfer learning for AI/ML development","description":"As part of your AI/ML process, you should evaluate using a pre-trained model and use transfer learning to avoid training a new model from scratch.","permalink":"/gsf-patterns/catalog/development/pre-trained-transfer-learning"},{"id":"catalog/architecture/compress-ml-models-for-inference","title":"Optimize the size of AI/ML models","description":"Large-scale AI/ML models require significant storage space and take more resources to run as compared to optimized models. Modern model optimization techniques can dramatically reduce model size and inference costs while maintaining accuracy.","permalink":"/gsf-patterns/catalog/architecture/compress-ml-models-for-inference"},{"id":"catalog/architecture/System Topology/energy-efficent-ai-edge","title":"Run AI models at the edge","description":"Data computation for ML workloads and ML inference is a significant contributor to the carbon footprint of the ML application. Also, if the ML model is running on the cloud, the data needs to be transferred and processed on the cloud to the required format that can be used by the ML model for inference.","permalink":"/gsf-patterns/catalog/architecture/System Topology/energy-efficent-ai-edge"},{"id":"catalog/architecture/Technology Selection/energy-efficent-framework","title":"Select a more energy efficient AI/ML framework","description":"Training an AI model implies a significant carbon footprint. The underlying framework used for the development, training, and deployment of AI/ML needs to be evaluated and considered to ensure the process is as energy efficient as possible.","permalink":"/gsf-patterns/catalog/architecture/Technology Selection/energy-efficent-framework"},{"id":"catalog/architecture/right-hardware-type","title":"Select the right hardware/VM instance types for AI/ML training","description":"Selecting the right hardware/VM instance types for AI/ML training and inference is critical for energy efficiency. The hardware landscape has evolved dramatically with specialized AI accelerators, GPUs, and custom silicon offering vastly different performance-per-watt characteristics.","permalink":"/gsf-patterns/catalog/architecture/right-hardware-type"},{"id":"catalog/architecture/Technology Selection/efficent-format-for-model-training","title":"Use efficient file formats for AI/ML development","description":"Efficient storage formats for both training data and model artifacts are essential to reduce storage costs, network bandwidth, and computational overhead in AI/ML development pipelines.","permalink":"/gsf-patterns/catalog/architecture/Technology Selection/efficent-format-for-model-training"},{"id":"catalog/architecture/Technology Selection/energy-efficent-models","title":"Use energy efficient AI/ML models","description":"Evaluate and use alternative, more energy efficient, models that provide similar functionality.","permalink":"/gsf-patterns/catalog/architecture/Technology Selection/energy-efficent-models"},{"id":"catalog/development/leverage-sustainable-regions","title":"Use sustainable regions for AI/ML training","description":"Depending on the model parameters and training iterations, training an AI/ML model consumes a lot of power and requires many servers which contribute to embodied emissions.","permalink":"/gsf-patterns/catalog/development/leverage-sustainable-regions"}],"unlisted":false}}')}}]);
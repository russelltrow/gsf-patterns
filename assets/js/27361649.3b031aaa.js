"use strict";(globalThis.webpackChunkgsf_docusaurus_template=globalThis.webpackChunkgsf_docusaurus_template||[]).push([[7429],{6451(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>o});const r=JSON.parse('{"id":"catalog/architecture/right-hardware-type","title":"Select the right hardware/VM instance types for AI/ML training","description":"Selecting the right hardware/VM instance types for AI/ML training and inference is critical for energy efficiency. The hardware landscape has evolved dramatically with specialized AI accelerators, GPUs, and custom silicon offering vastly different performance-per-watt characteristics.","source":"@site/docs/catalog/architecture/right-hardware-type.md","sourceDirName":"catalog/architecture","slug":"/catalog/architecture/right-hardware-type","permalink":"/gsf-patterns/catalog/architecture/right-hardware-type","draft":false,"unlisted":false,"editUrl":"https://github.com/Green-Software-Foundation/patterns/edit/main/docs/catalog/architecture/right-hardware-type.md","tags":[{"inline":true,"label":"ai","permalink":"/gsf-patterns/tags/ai"},{"inline":true,"label":"machine-learning","permalink":"/gsf-patterns/tags/machine-learning"},{"inline":true,"label":"cloud","permalink":"/gsf-patterns/tags/cloud"},{"inline":true,"label":"compute","permalink":"/gsf-patterns/tags/compute"},{"inline":true,"label":"role:data-scientist","permalink":"/gsf-patterns/tags/role-data-scientist"},{"inline":true,"label":"role:cloud-engineer","permalink":"/gsf-patterns/tags/role-cloud-engineer"},{"inline":true,"label":"role:software-engineer","permalink":"/gsf-patterns/tags/role-software-engineer"},{"inline":true,"label":"size:large","permalink":"/gsf-patterns/tags/size-large"}],"version":"current","frontMatter":{"version":1,"submitted_by":"navveenb","published_date":"2022-11-10T00:00:00.000Z","category":"architecture","description":"Selecting the right hardware/VM instance types for AI/ML training and inference is critical for energy efficiency. The hardware landscape has evolved dramatically with specialized AI accelerators, GPUs, and custom silicon offering vastly different performance-per-watt characteristics.","tags":["ai","machine-learning","cloud","compute","role:data-scientist","role:cloud-engineer","role:software-engineer","size:large"]},"sidebar":"tutorialSidebar","previous":{"title":"Reduce network traversal between VMs","permalink":"/gsf-patterns/catalog/architecture/reduce-network-traversal-between-VMs"},"next":{"title":"Scale logical components independently","permalink":"/gsf-patterns/catalog/architecture/scale-logical-components-independently"}}');var s=i(4848),l=i(8453);const t={version:1,submitted_by:"navveenb",published_date:new Date("2022-11-10T00:00:00.000Z"),category:"architecture",description:"Selecting the right hardware/VM instance types for AI/ML training and inference is critical for energy efficiency. The hardware landscape has evolved dramatically with specialized AI accelerators, GPUs, and custom silicon offering vastly different performance-per-watt characteristics.",tags:["ai","machine-learning","cloud","compute","role:data-scientist","role:cloud-engineer","role:software-engineer","size:large"]},a="Select the right hardware/VM instance types for AI/ML training",c={},o=[{value:"Description",id:"description",level:2},{value:"Solution",id:"solution",level:2},{value:"Modern GPU Hardware (2025)",id:"modern-gpu-hardware-2025",level:3},{value:"Custom Silicon and Cloud TPUs",id:"custom-silicon-and-cloud-tpus",level:3},{value:"Decision Matrix for Hardware Selection",id:"decision-matrix-for-hardware-selection",level:3},{value:"Energy Efficiency Metrics (2025 Benchmarks)",id:"energy-efficiency-metrics-2025-benchmarks",level:3},{value:"Modern Workload Examples and Patterns",id:"modern-workload-examples-and-patterns",level:3},{value:"SCI Impact",id:"sci-impact",level:2},{value:"Assumptions",id:"assumptions",level:2},{value:"Considerations",id:"considerations",level:2},{value:"Hardware Selection Criteria:",id:"hardware-selection-criteria",level:3},{value:"Red Flags to Avoid:",id:"red-flags-to-avoid",level:3},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"select-the-right-hardwarevm-instance-types-for-aiml-training",children:"Select the right hardware/VM instance types for AI/ML training"})}),"\n",(0,s.jsx)(n.h2,{id:"description",children:"Description"}),"\n",(0,s.jsx)(n.p,{children:"Training an AI model has a significant carbon footprint. Selecting the right hardware/VM instance types for training and inference is one of the most impactful choices you can make as part of your energy-efficient AI/ML process. The hardware landscape has evolved dramatically since 2022, with a proliferation of specialized AI accelerators, GPU generations, and custom silicon options, each with vastly different performance-per-watt characteristics and suitability for different workloads."}),"\n",(0,s.jsx)(n.h2,{id:"solution",children:"Solution"}),"\n",(0,s.jsx)(n.p,{children:"Evaluate and select hardware based on your specific workload requirements, balancing performance, energy efficiency, cost, and availability:"}),"\n",(0,s.jsx)(n.h3,{id:"modern-gpu-hardware-2025",children:"Modern GPU Hardware (2025)"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NVIDIA GPUs (Market Leader):"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.em,{children:"Training-Optimized:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"H100/H200 (Hopper architecture)"}),": Flagship for large-scale training (70B+ parameter models)"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"60-80 TFLOPS FP16, ~700W TDP"}),"\n",(0,s.jsx)(n.li,{children:"Best for: Distributed training of foundation models, massive batch sizes"}),"\n",(0,s.jsx)(n.li,{children:"TCO: High upfront cost, excellent performance-per-watt for large workloads"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"L40S (Ada Lovelace)"}),": Versatile for training and inference"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"45 TFLOPS FP16, 350W TDP"}),"\n",(0,s.jsx)(n.li,{children:"Best for: Mid-size models (7B-30B), mixed training/inference workloads"}),"\n",(0,s.jsx)(n.li,{children:"Excellent balance of performance and power efficiency"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.em,{children:"Inference-Optimized:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"L4 (Ada Lovelace)"}),": Efficient inference and fine-tuning","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"30 TFLOPS FP16, 72W TDP"}),"\n",(0,s.jsx)(n.li,{children:"Best for: Inference serving, LoRA/QLoRA fine-tuning, edge deployment"}),"\n",(0,s.jsx)(n.li,{children:"Outstanding power efficiency for inference workloads"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.em,{children:"Consumer/Development:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RTX 6000 Ada"}),": Workstation-class for research and development","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"48GB memory, good for prototyping 7B-13B models"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"AMD GPUs (Growing Ecosystem):"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"MI300X"}),": Competitive with H100 for LLM training","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"192GB HBM3, excellent for large model training"}),"\n",(0,s.jsx)(n.li,{children:"Best for: Organizations diversifying from NVIDIA"}),"\n",(0,s.jsx)(n.li,{children:"Mature ROCm software stack for PyTorch/TensorFlow"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"MI300/MI250"}),": Previous generation, cost-effective for certain workloads"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Intel GPUs:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Gaudi2/Gaudi3"}),": Purpose-built AI accelerators","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Competitive pricing vs. NVIDIA"}),"\n",(0,s.jsx)(n.li,{children:"Growing software ecosystem"}),"\n",(0,s.jsx)(n.li,{children:"Best for: Cost-sensitive large-scale training"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"custom-silicon-and-cloud-tpus",children:"Custom Silicon and Cloud TPUs"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Google Cloud TPUs:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"TPU v5e/v5p"}),": 5th generation, optimized for LLM training and inference","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"TPU v5e: Cost-optimized for training and inference"}),"\n",(0,s.jsx)(n.li,{children:"TPU v5p: Highest performance for cutting-edge research"}),"\n",(0,s.jsx)(n.li,{children:"Excellent for JAX-based training (native framework)"}),"\n",(0,s.jsx)(n.li,{children:"2-3x better performance-per-watt than comparable GPUs for certain workloads"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"AWS Custom Silicon:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Trainium (Trn1)"}),": Purpose-built for training"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Up to 40% better price-performance than GPUs for LLM training"}),"\n",(0,s.jsx)(n.li,{children:"Best for: Large-scale training on AWS infrastructure"}),"\n",(0,s.jsx)(n.li,{children:"Supported by PyTorch and NeuronSDK"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Inferentia2 (Inf2)"}),": Optimized for inference"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"10x better throughput vs. GPU inference for similar cost"}),"\n",(0,s.jsx)(n.li,{children:"Best for: High-volume inference serving, chatbots, embeddings"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Emerging Specialized Hardware:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Cerebras WSE-3"}),": Wafer-scale engine for massive models"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Entire wafer as single chip, 900,000 cores"}),"\n",(0,s.jsx)(n.li,{children:"Best for: Research institutions, extreme-scale models"}),"\n",(0,s.jsx)(n.li,{children:"Unique architecture for sparse models"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"SambaNova DataScale"}),": Reconfigurable dataflow architecture"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Efficient for training and inference"}),"\n",(0,s.jsx)(n.li,{children:"Growing enterprise adoption"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"decision-matrix-for-hardware-selection",children:"Decision Matrix for Hardware Selection"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"By Model Size:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"<1B parameters"}),": CPU or single consumer GPU (RTX 4090, L4)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"1B-7B parameters"}),": Single L4, L40S, or A100"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"7B-30B parameters"}),": L40S, A100, H100, MI300X"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"30B-70B parameters"}),": H100, MI300X, multi-GPU setup, or TPU v5e"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"70B+ parameters"}),": H100/H200 multi-node, TPU v5p, or Trainium clusters"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"By Workload Type:"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.em,{children:"Pre-training from scratch:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"H100/H200 for maximum speed"}),"\n",(0,s.jsx)(n.li,{children:"TPU v5e/v5p for cost-efficiency at scale"}),"\n",(0,s.jsx)(n.li,{children:"Trainium for AWS-native workflows"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.em,{children:"Fine-tuning:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LoRA/QLoRA (parameter-efficient)"}),": L4, single A100, consumer GPUs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Full fine-tuning"}),": Same as pre-training but smaller scale"]}),"\n",(0,s.jsx)(n.li,{children:"Consider spot instances for cost savings"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.em,{children:"Inference serving:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"High-throughput"}),": Inferentia2, L4 clusters"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Low-latency"}),": L4, L40S with TensorRT or vLLM optimization"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Edge deployment"}),": Quantized models on CPU or mobile accelerators"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"By Cost Profile:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Budget-conscious"}),": AMD MI-series, Intel Gaudi, Trainium"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance-critical"}),": NVIDIA H100/H200"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Balanced"}),": L40S, TPU v5e"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Development"}),": Consumer GPUs (RTX series) or L4"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"energy-efficiency-metrics-2025-benchmarks",children:"Energy Efficiency Metrics (2025 Benchmarks)"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"TFLOPS per Watt (FP16 Training):"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"L4: ~417 TFLOPS/watt (30 TFLOPS / 72W) - Most efficient for inference"}),"\n",(0,s.jsx)(n.li,{children:"L40S: ~129 TFLOPS/watt (45 TFLOPS / 350W)"}),"\n",(0,s.jsx)(n.li,{children:"H100: ~86-114 TFLOPS/watt (60-80 TFLOPS / 700W)"}),"\n",(0,s.jsx)(n.li,{children:"TPU v5e: ~100-150 TFLOPS/watt (estimated, workload-dependent)"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Total Cost of Ownership (TCO):"}),"\nConsider:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Initial hardware or hourly cloud cost"}),"\n",(0,s.jsx)(n.li,{children:"Power consumption ($/kWh \xd7 Watts \xd7 training hours)"}),"\n",(0,s.jsx)(n.li,{children:"Cooling requirements (typically 1.5x power consumption)"}),"\n",(0,s.jsx)(n.li,{children:"Embodied carbon of manufacturing"}),"\n",(0,s.jsx)(n.li,{children:"Utilization rates and idle power"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"modern-workload-examples-and-patterns",children:"Modern Workload Examples and Patterns"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Training Scenarios:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"7B model training"}),": 8x L40S or 4x H100, ~2-4 weeks on typical datasets"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"70B model training"}),": 64-128x H100 or TPU v5p pod, weeks to months"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LoRA fine-tuning of 7B model"}),": Single L4 or A100, hours to days"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Distributed Training Orchestration:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"DeepSpeed: Multi-node training with ZeRO optimizer"}),"\n",(0,s.jsx)(n.li,{children:"FSDP (PyTorch): Fully Sharded Data Parallel for large models"}),"\n",(0,s.jsx)(n.li,{children:"Megatron-LM: NVIDIA's framework for massive models"}),"\n",(0,s.jsx)(n.li,{children:"Enable training models larger than single-GPU memory"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Inference Optimization:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"vLLM: High-throughput inference with PagedAttention"}),"\n",(0,s.jsx)(n.li,{children:"TensorRT-LLM: NVIDIA's optimized inference engine"}),"\n",(0,s.jsx)(n.li,{children:"Text Generation Inference (TGI): HuggingFace's production server"}),"\n",(0,s.jsx)(n.li,{children:"Batch multiple requests to maximize GPU utilization (10-100x better throughput)"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Cost Optimization Strategies:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Spot/preemptible instances"}),": 60-90% savings for interruptible training"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reserved instances"}),": 30-50% savings for predictable workloads"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Mixed precision training"}),": FP16/BF16 for 2x speedup with minimal accuracy loss"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Gradient accumulation"}),": Simulate large batches on smaller GPUs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Checkpointing"}),": Resume training after interruptions (essential for spot instances)"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"sci-impact",children:"SCI Impact"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"SCI = (E * I) + M per R"}),"\n",(0,s.jsx)(n.a,{href:"https://grnsft.org/sci",children:"Software Carbon Intensity Spec"})]}),"\n",(0,s.jsx)(n.p,{children:"Selecting the right hardware/VM types impacts SCI as follows:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"E"}),": Energy-efficient hardware reduces electricity consumption through:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Higher TFLOPS-per-watt for actual workload"}),"\n",(0,s.jsx)(n.li,{children:"Lower idle power consumption"}),"\n",(0,s.jsx)(n.li,{children:"Better memory bandwidth efficiency"}),"\n",(0,s.jsx)(n.li,{children:"Optimized tensor operations for AI workloads"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"M"}),": Reduces embodied carbon by:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Requiring fewer total accelerators for same workload"}),"\n",(0,s.jsx)(n.li,{children:"Shorter training times reducing resource utilization"}),"\n",(0,s.jsx)(n.li,{children:"Efficient inference enables running on smaller infrastructure"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"assumptions",children:"Assumptions"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Cloud provider offers appropriate hardware in your target regions"}),"\n",(0,s.jsx)(n.li,{children:"Software frameworks support the selected hardware (drivers, SDKs)"}),"\n",(0,s.jsx)(n.li,{children:"Workload can be optimized for the hardware architecture"}),"\n",(0,s.jsx)(n.li,{children:"Budget allows for energy-efficient hardware (which often has higher upfront cost)"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"considerations",children:"Considerations"}),"\n",(0,s.jsx)(n.h3,{id:"hardware-selection-criteria",children:"Hardware Selection Criteria:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Performance:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Peak TFLOPS less important than sustained performance for your specific model architecture"}),"\n",(0,s.jsx)(n.li,{children:"Memory capacity and bandwidth critical for large models"}),"\n",(0,s.jsx)(n.li,{children:"Interconnect speed matters for multi-GPU training"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Software Ecosystem:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"NVIDIA"}),": Most mature software stack (CUDA, cuDNN, TensorRT)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AMD"}),": Growing PyTorch support via ROCm"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"TPU"}),": Best with JAX, good with PyTorch/XLA"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AWS Trainium"}),": Requires NeuronSDK, growing PyTorch support"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Availability and Cost:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"GPU availability has improved since 2022-2023 shortage"}),"\n",(0,s.jsx)(n.li,{children:"Cloud region selection affects both availability and carbon intensity"}),"\n",(0,s.jsx)(n.li,{children:"Consider carbon-aware scheduling (train in low-carbon regions/times)"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Future-Proofing:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Rapid hardware evolution means 2-3 year refresh cycles"}),"\n",(0,s.jsx)(n.li,{children:"Design training pipelines to be hardware-agnostic when possible"}),"\n",(0,s.jsx)(n.li,{children:"Use frameworks that abstract hardware (PyTorch, JAX, TensorFlow)"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"red-flags-to-avoid",children:"Red Flags to Avoid:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Over-provisioning: Using H100 for workloads that run fine on L4"}),"\n",(0,s.jsx)(n.li,{children:"Under-provisioning: Insufficient memory causing excessive swapping"}),"\n",(0,s.jsx)(n.li,{children:"Ignoring power efficiency for long-running training jobs"}),"\n",(0,s.jsx)(n.li,{children:"Not considering spot instances for fault-tolerant workloads"}),"\n",(0,s.jsx)(n.li,{children:"Using outdated hardware (V100, P100) when efficient alternatives exist"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/1906.02243.pdf",children:"Energy and Policy Considerations for Deep Learning in NLP"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://resources.nvidia.com/en-us-tensor-core",children:"NVIDIA H100 Tensor Core GPU Architecture"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html",children:"AMD MI300X Architecture"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://cloud.google.com/tpu/docs/v5e",children:"Google TPU v5e Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://aws.amazon.com/machine-learning/trainium/",children:"AWS Trainium"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://aws.amazon.com/machine-learning/inferentia/",children:"AWS Inferentia2"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/vllm-project/vllm",children:"vLLM - Fast LLM Inference"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.deepspeed.ai/",children:"DeepSpeed for Distributed Training"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://grnsft.org/sci",children:"Software Carbon Intensity Spec"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>t,x:()=>a});var r=i(6540);const s={},l=r.createContext(s);function t(e){const n=r.useContext(l);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),r.createElement(l.Provider,{value:n},e.children)}}}]);
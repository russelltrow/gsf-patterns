"use strict";(globalThis.webpackChunkgsf_docusaurus_template=globalThis.webpackChunkgsf_docusaurus_template||[]).push([[7180],{1654(e,n,i){i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"catalog/architecture/Technology Selection/efficent-format-for-model-training","title":"Use efficient file formats for AI/ML development","description":"Efficient storage formats for both training data and model artifacts are essential to reduce storage costs, network bandwidth, and computational overhead in AI/ML development pipelines.","source":"@site/docs/catalog/architecture/Technology Selection/efficent-format-for-model-training.md","sourceDirName":"catalog/architecture/Technology Selection","slug":"/catalog/architecture/Technology Selection/efficent-format-for-model-training","permalink":"/gsf-patterns/catalog/architecture/Technology Selection/efficent-format-for-model-training","draft":false,"unlisted":false,"editUrl":"https://github.com/Green-Software-Foundation/patterns/edit/main/docs/catalog/architecture/Technology Selection/efficent-format-for-model-training.md","tags":[{"inline":true,"label":"ai","permalink":"/gsf-patterns/tags/ai"},{"inline":true,"label":"machine-learning","permalink":"/gsf-patterns/tags/machine-learning"},{"inline":true,"label":"storage","permalink":"/gsf-patterns/tags/storage"},{"inline":true,"label":"role:data-scientist","permalink":"/gsf-patterns/tags/role-data-scientist"},{"inline":true,"label":"role:software-engineer","permalink":"/gsf-patterns/tags/role-software-engineer"},{"inline":true,"label":"size:medium","permalink":"/gsf-patterns/tags/size-medium"}],"version":"current","frontMatter":{"version":1,"submitted_by":"navveenb","published_date":"2022-11-10T00:00:00.000Z","category":"architecture","description":"Efficient storage formats for both training data and model artifacts are essential to reduce storage costs, network bandwidth, and computational overhead in AI/ML development pipelines.","tags":["ai","machine-learning","storage","role:data-scientist","role:software-engineer","size:medium"]},"sidebar":"tutorialSidebar","previous":{"title":"Run AI models at the edge","permalink":"/gsf-patterns/catalog/architecture/System Topology/energy-efficent-ai-edge"},"next":{"title":"Select a more energy efficient AI/ML framework","permalink":"/gsf-patterns/catalog/architecture/Technology Selection/energy-efficent-framework"}}');var s=i(4848),t=i(8453);const a={version:1,submitted_by:"navveenb",published_date:new Date("2022-11-10T00:00:00.000Z"),category:"architecture",description:"Efficient storage formats for both training data and model artifacts are essential to reduce storage costs, network bandwidth, and computational overhead in AI/ML development pipelines.",tags:["ai","machine-learning","storage","role:data-scientist","role:software-engineer","size:medium"]},l="Use efficient file formats for AI/ML development",o={},d=[{value:"Description",id:"description",level:2},{value:"Solution",id:"solution",level:2},{value:"Training Data Formats",id:"training-data-formats",level:3},{value:"Model Artifact Formats",id:"model-artifact-formats",level:3},{value:"Model Versioning and Metadata",id:"model-versioning-and-metadata",level:3},{value:"Compression Strategies",id:"compression-strategies",level:3},{value:"SCI Impact",id:"sci-impact",level:2},{value:"Assumptions",id:"assumptions",level:2},{value:"Considerations",id:"considerations",level:2},{value:"Performance Benchmarks (2025 Typical Workloads)",id:"performance-benchmarks-2025-typical-workloads",level:3},{value:"References",id:"references",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"use-efficient-file-formats-for-aiml-development",children:"Use efficient file formats for AI/ML development"})}),"\n",(0,s.jsx)(n.h2,{id:"description",children:"Description"}),"\n",(0,s.jsx)(n.p,{children:"Data processing and storage constitute a significant portion of AI/ML development and impact the carbon footprint of your application. Variety and volumes of data might need to be captured and pre-processed for building ML models. Efficient storage of both training data and model artifacts becomes extremely important to reduce storage space, network transfer costs, and memory consumption during ML development."}),"\n",(0,s.jsx)(n.h2,{id:"solution",children:"Solution"}),"\n",(0,s.jsx)(n.p,{children:"Use efficient file formats optimized for AI/ML workloads across both training data and model storage:"}),"\n",(0,s.jsx)(n.h3,{id:"training-data-formats",children:"Training Data Formats"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Parquet (Columnar Storage):"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Industry standard for structured/tabular data"}),"\n",(0,s.jsx)(n.li,{children:"Efficient compression and column-oriented storage"}),"\n",(0,s.jsx)(n.li,{children:"5-10x smaller than CSV with faster read performance"}),"\n",(0,s.jsx)(n.li,{children:"Excellent for feature engineering and batch processing"}),"\n",(0,s.jsx)(n.li,{children:"Supported by all major data processing frameworks (Spark, Pandas, Dask)"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"HuggingFace Datasets (Arrow Format):"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Now the de facto standard for ML dataset storage (2025)"}),"\n",(0,s.jsx)(n.li,{children:"Memory-mapped for efficient random access"}),"\n",(0,s.jsx)(n.li,{children:"Zero-copy reads reduce memory overhead"}),"\n",(0,s.jsx)(n.li,{children:"Built-in streaming support for large datasets"}),"\n",(0,s.jsx)(n.li,{children:"Integrated with popular ML frameworks"}),"\n",(0,s.jsx)(n.li,{children:"Examples: Most open-source LLM training datasets (RedPajama, The Pile, etc.)"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Zarr (Multi-dimensional Arrays):"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Optimized for large multi-dimensional numerical arrays"}),"\n",(0,s.jsx)(n.li,{children:"Chunked storage enables parallel I/O"}),"\n",(0,s.jsx)(n.li,{children:"Excellent for scientific computing and large-scale vision datasets"}),"\n",(0,s.jsx)(n.li,{children:"Cloud-native with support for object storage backends"}),"\n",(0,s.jsx)(n.li,{children:"Ideal for medical imaging, satellite imagery, and climate data"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"WebDataset:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"TAR-based format optimized for streaming during training"}),"\n",(0,s.jsx)(n.li,{children:"Efficient for large-scale distributed training"}),"\n",(0,s.jsx)(n.li,{children:"Minimal overhead for sequential access patterns"}),"\n",(0,s.jsx)(n.li,{children:"Used by many large-scale vision models (LAION datasets)"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"model-artifact-formats",children:"Model Artifact Formats"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"SafeTensors (2025 Standard):"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Now the default format for HuggingFace model distribution"}),"\n",(0,s.jsx)(n.li,{children:"Safe deserialization (no arbitrary code execution risks)"}),"\n",(0,s.jsx)(n.li,{children:"Fast loading with memory mapping"}),"\n",(0,s.jsx)(n.li,{children:"Single file format simplifying model distribution"}),"\n",(0,s.jsx)(n.li,{children:"Examples: LLaMA 2, LLaMA 3, Mistral 7B, Phi-3 all distributed in SafeTensors"}),"\n",(0,s.jsx)(n.li,{children:"2-3x faster loading than legacy pickle formats"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"ONNX (Open Neural Network Exchange):"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Cross-framework model interoperability format"}),"\n",(0,s.jsx)(n.li,{children:"Enables deployment optimization (TensorRT, OpenVINO)"}),"\n",(0,s.jsx)(n.li,{children:"Hardware-agnostic model representation"}),"\n",(0,s.jsx)(n.li,{children:"Supports quantization and graph optimization"}),"\n",(0,s.jsx)(n.li,{children:"Ideal for production deployment pipelines"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"MLX Format (Apple Silicon):"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Optimized for Apple M-series chips"}),"\n",(0,s.jsx)(n.li,{children:"Efficient use of unified memory architecture"}),"\n",(0,s.jsx)(n.li,{children:"Growing ecosystem for on-device AI"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Modern Checkpointing:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"DeepSpeed checkpoints"}),": Efficient for distributed training of large models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"FSDP (Fully Sharded Data Parallel)"}),": PyTorch's distributed checkpointing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"TensorFlow SavedModel"}),": Comprehensive format including graph and weights"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"model-versioning-and-metadata",children:"Model Versioning and Metadata"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Model Registry Patterns:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Track model lineage, parameters, and training configurations"}),"\n",(0,s.jsx)(n.li,{children:"Store metadata alongside model artifacts"}),"\n",(0,s.jsx)(n.li,{children:"Version control for models (MLflow, DVC, Weights & Biases)"}),"\n",(0,s.jsx)(n.li,{children:"Delta/diff storage for model versions to save space"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Metadata Tracking:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Parameter counts and architecture specifications"}),"\n",(0,s.jsx)(n.li,{children:"Training dataset provenance"}),"\n",(0,s.jsx)(n.li,{children:"Performance metrics and benchmarks"}),"\n",(0,s.jsx)(n.li,{children:"Quantization and optimization settings"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"compression-strategies",children:"Compression Strategies"}),"\n",(0,s.jsx)(n.p,{children:"Apply compression to further reduce storage and transfer costs:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"For training data:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"gzip"}),": Widely supported, moderate compression (2-3x)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"zstd"})," (Zstandard): Better compression ratio and speed than gzip (3-5x)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Snappy"}),": Fast compression for streaming scenarios"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"For model artifacts:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'Quantization (see "Optimize the size of AI/ML models" pattern)'}),"\n",(0,s.jsx)(n.li,{children:"Sparse tensor formats for pruned models"}),"\n",(0,s.jsx)(n.li,{children:"Compression-aware storage formats"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"sci-impact",children:"SCI Impact"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"SCI = (E * I) + M per R"}),"\n",(0,s.jsx)(n.a,{href:"https://grnsft.org/sci",children:"Software Carbon Intensity Spec"})]}),"\n",(0,s.jsx)(n.p,{children:"Using efficient file formats for ML development impacts SCI as follows:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"E"}),": Reduces energy consumption through:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"More efficient data storage and retrieval (less disk I/O)"}),"\n",(0,s.jsx)(n.li,{children:"Reduced network bandwidth for model distribution"}),"\n",(0,s.jsx)(n.li,{children:"Lower memory usage during training and inference"}),"\n",(0,s.jsx)(n.li,{children:"Faster loading times reducing idle GPU cycles"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"M"}),": Reduces embodied carbon by:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Requiring less storage infrastructure"}),"\n",(0,s.jsx)(n.li,{children:"Enabling training on smaller memory systems"}),"\n",(0,s.jsx)(n.li,{children:"Reducing data center storage capacity needs"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"assumptions",children:"Assumptions"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Storage format compatibility with your ML framework and infrastructure"}),"\n",(0,s.jsx)(n.li,{children:"Training pipeline can leverage memory-mapped or streaming data access"}),"\n",(0,s.jsx)(n.li,{children:"Model distribution systems support modern formats (SafeTensors, ONNX)"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"considerations",children:"Considerations"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Format Selection Criteria:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Training data"}),": Choose based on access pattern (random vs. sequential) and data structure"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model artifacts"}),": Prioritize SafeTensors for safety and performance, ONNX for deployment flexibility"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Migration Path"}),": Convert existing datasets and models incrementally"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Backward Compatibility"}),": Maintain legacy format support during transition periods"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Storage Backend"}),": Some formats (Zarr, Parquet) work especially well with cloud object storage"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Compression Tradeoff"}),": Balance compression ratio vs. decompression speed based on use case"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Streaming Requirements"}),": Use WebDataset or HuggingFace Datasets streaming for data too large for local storage"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"performance-benchmarks-2025-typical-workloads",children:"Performance Benchmarks (2025 Typical Workloads)"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Training Data:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Parquet: 5-10x smaller than CSV, 3-5x faster loading"}),"\n",(0,s.jsx)(n.li,{children:"HuggingFace Datasets: 2-3x faster than loading from disk with zero-copy reads"}),"\n",(0,s.jsx)(n.li,{children:"WebDataset: Near-zero overhead for streaming large-scale datasets"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Model Artifacts:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"SafeTensors: 2-3x faster loading than pickle-based formats"}),"\n",(0,s.jsx)(n.li,{children:"ONNX with optimization: 10-50% inference speedup vs. native frameworks"}),"\n",(0,s.jsx)(n.li,{children:"Quantized formats: 50-75% size reduction with <5% accuracy loss"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://parquet.apache.org/",children:"Apache Parquet"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/datasets/",children:"HuggingFace Datasets Library"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/safetensors/",children:"SafeTensors Format"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://onnx.ai/",children:"ONNX - Open Neural Network Exchange"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://zarr.dev/",children:"Zarr - Chunked N-dimensional Arrays"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/webdataset/webdataset",children:"WebDataset"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/ml-explore/mlx",children:"MLX - Apple Machine Learning Framework"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.deepspeed.ai/tutorials/checkpoint/",children:"DeepSpeed Checkpointing"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://grnsft.org/sci",children:"Software Carbon Intensity Spec"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453(e,n,i){i.d(n,{R:()=>a,x:()=>l});var r=i(6540);const s={},t=r.createContext(s);function a(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);